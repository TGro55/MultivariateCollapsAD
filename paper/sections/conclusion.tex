Computing differential operators is a critical component in scientific machine learning, particularly for physics-informed neural networks and variational Monte Carlo. 
Our work introduces collapsed Taylor-mode AD, a simple yet effective optimization that propagates the sum of highest-order coefficients up the computational graph. This approach encompasses recent advances in forward-mode schemes, recovering the forward Laplacian, while being applicable to stochastic Taylor mode. General PDE operators are treated by a transformation into a form amenable to collapsing highest-order Taylor coefficients. We empirically demonstrate speedups and memory savings for computing (randomized) Laplacians and Biharmonic operators in accordance with our theoretical findings. As the optimizations are achieved through simple graph rewrites based on linearity, in principle allowing for an integration into existing just-in-time compilers without requiring a specialized interface.

We believe this work takes an important step towards making Taylor mode a practical alternative in scientific machine learning, while maintaining ease of use through potential compiler integration. 
Future work could focus on integrating these optimizations directly into ML framework compilers, extending support to more primitive operations, and exploring additional graph-based optimizations for automatic differentiation.